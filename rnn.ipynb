{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "a0N10W1Iz7w-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Single cell RNN**"
      ],
      "metadata": {
        "id": "imlcP-9GW8wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torch.optim as optim\n",
        "\n",
        "#hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 16\n",
        "max_iters = 5001\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "split_ratio = 0.9\n",
        "rnn_hidden_size = 512\n",
        "chunk_size = block_size + 1\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('/content/cfg3b.txt', 'r', encoding='utf-8') as fp: #ISO-8859-1 para español\n",
        "    text = fp.read()\n",
        "\n",
        "char_set = set(text)\n",
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "\n",
        "text_encoded = np.array(\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32)\n",
        "\n",
        "text_chunks = [text_encoded[i:i+chunk_size]\n",
        "               for i in range(len(text_encoded)-chunk_size+1)]\n",
        "\n",
        "split_index = int(split_ratio * len(text_chunks))\n",
        "train_data, val_data = text_chunks[:split_index], text_chunks[split_index:]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "train_dataset = TextDataset(torch.tensor(np.array(train_data)))\n",
        "val_dataset = TextDataset(torch.tensor(np.array(val_data)))\n",
        "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_dl = DataLoader(val_dataset, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, n_embd) #4x384\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(n_embd, rnn_hidden_size,\n",
        "                           batch_first=True) #batch_first parameter is set to True\n",
        "                           #to indicate that the input data has batch size as the first dimension\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    #x: The input sequence represented as integer-encoded words\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1) #(64,1,384)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell)) #(64,1,512)\n",
        "        out = self.fc(out).reshape(out.size(0), -1) #(64, 4)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size) #(1, 64, 512)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)\n",
        "\n",
        "vocab_size = len(char_array)\n",
        "model = RNN(vocab_size, n_embd, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(max_iters):\n",
        "  #if epoch % eval_interval == 0:\n",
        "    #Validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    #train_loss = 0\n",
        "    with torch.no_grad():\n",
        "        val_hidden, val_cell = model.init_hidden(batch_size)\n",
        "        val_seq_batch, val_target_batch = next(iter(val_dl)) #64x16 tensors\n",
        "        val_seq_batch = val_seq_batch.to(device)\n",
        "        val_target_batch = val_target_batch.to(device)\n",
        "        for c in range(block_size): #tengo 16 tensores de 64\n",
        "            val_pred, val_hidden, val_cell = model(val_seq_batch[:, c], val_hidden, val_cell)\n",
        "            val_loss += loss_fn(val_pred, val_target_batch[:, c])\n",
        "        val_loss = val_loss.item()/block_size\n",
        "\n",
        "    #Training set\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(train_dl))\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    for c in range(block_size):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        train_loss += loss_fn(pred, target_batch[:, c])\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss = train_loss.item()/block_size\n",
        "    #print(f'Epoch {epoch} - Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}')\n",
        "    if epoch % eval_interval == 0:\n",
        "        print(f'Epoch {epoch} - Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "def sample(model, starting_str,\n",
        "           len_generated_text=512,\n",
        "           scale_factor=1.0):\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden = hidden.to(device)\n",
        "    cell = cell.to(device)\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    last_char = last_char.to(device)\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        if str(char_array[last_char]) == \" \":\n",
        "          return generated_str\n",
        "        generated_str += str(char_array[last_char])\n",
        "        last_char = last_char.to(device)\n",
        "\n",
        "    return generated_str"
      ],
      "metadata": {
        "id": "FgUjQ0OzXF7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418ea3f5-027d-45b1-e862-6772fca70fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.842692 M parameters\n",
            "Epoch 0 - Train Loss: 1.3832 | Validation Loss: 1.3830\n",
            "Epoch 500 - Train Loss: 1.2393 | Validation Loss: 1.2330\n",
            "Epoch 1000 - Train Loss: 1.1204 | Validation Loss: 1.1181\n",
            "Epoch 1500 - Train Loss: 1.0408 | Validation Loss: 1.0289\n",
            "Epoch 2000 - Train Loss: 0.9774 | Validation Loss: 0.9595\n",
            "Epoch 2500 - Train Loss: 0.9229 | Validation Loss: 0.9046\n",
            "Epoch 3000 - Train Loss: 0.8886 | Validation Loss: 0.8610\n",
            "Epoch 3500 - Train Loss: 0.8569 | Validation Loss: 0.8266\n",
            "Epoch 4000 - Train Loss: 0.8354 | Validation Loss: 0.7991\n",
            "Epoch 4500 - Train Loss: 0.8401 | Validation Loss: 0.7756\n",
            "Epoch 5000 - Train Loss: 0.7684 | Validation Loss: 0.7542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def aVerQuePlan(text):\n",
        "  listTokens = text.split()\n",
        "  setTokens = set(listTokens)\n",
        "  print(len(listTokens))\n",
        "  print(len(setTokens))\n",
        "\n",
        "aVerQuePlan(text)\n",
        "#print(f'Número de frases: {frases} \\n Número de frases diferentes: {frasesDif}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrOgZ1J5taaM",
        "outputId": "a8e1f94a-c983-4f04-9d69-fbbaa4a31b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7000\n",
            "7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, starting_str,\n",
        "           len_generated_text=512,\n",
        "           scale_factor=1.0):\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden = hidden.to(device)\n",
        "    cell = cell.to(device)\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    last_char = last_char.to(device)\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        #if str(char_array[last_char]) == \" \":\n",
        "        #  return generated_str\n",
        "        generated_str += str(char_array[last_char])\n",
        "        last_char = last_char.to(device)\n",
        "\n",
        "    return generated_str\n",
        "model.to(device)\n",
        "gen_text = \"\"\n",
        "for i in range (500):\n",
        "  gen_text += sample(model, starting_str=' ')\n",
        "#print(gen_text)"
      ],
      "metadata": {
        "id": "WP8mppHiCLh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición de CFGs\n",
        "import nltk\n",
        "from nltk import CFG\n",
        "from nltk.util import ngrams\n",
        "from nltk.parse.generate import generate\n",
        "from nltk.parse import RecursiveDescentParser\n",
        "\n",
        "cfg3b = CFG.fromstring(\"\"\"\n",
        "    22 -> 21 20 | 20 19\n",
        "    21 -> 18 16 | 16 18 17\n",
        "    20 -> 16 17 | 17 16 18\n",
        "    19 -> 17 18 16 | 16 17 18\n",
        "    18 -> 14 13 | 15 14 13\n",
        "    17 -> 15 13 14 | 14 13 15\n",
        "    16 -> 13 15 14 | 15 13\n",
        "    15 -> 11 12 10 | 12 11 10\n",
        "    14 -> 10 11 12 | 11 10 12\n",
        "    13 -> 12 11 | 11 12\n",
        "    12 -> 9 7 8 | 8 9 7\n",
        "    11 -> 7 8 9 | 8 7 9\n",
        "    10 -> 9 8 7 | 7 9 8\n",
        "    9 -> '2' '1' | '3' '2' '1'\n",
        "    8 -> '3' '1' '2' | '3' '2'\n",
        "    7 -> '1' '2' '3' | '3' '1'\n",
        "\"\"\")\n",
        "\n",
        "cfg3i = CFG.fromstring(\"\"\"\n",
        "    22 -> 21 20 19 | 19 19 20\n",
        "    21 -> 18 17 | 16 16 18\n",
        "    20 -> 18 18 | 17 16 17\n",
        "    19 -> 16 16 | 18 16 18\n",
        "    18 -> 14 15 | 14 15 13\n",
        "    17 -> 15 14 | 15 15\n",
        "    16 -> 14 14 | 13 13\n",
        "    15 -> 11 10 12 | 11 11 10\n",
        "    14 -> 10 10 | 10 10 10\n",
        "    13 -> 10 12 11 | 12 11\n",
        "    12 -> 8 7 | 7 9 9\n",
        "    11 -> 7 7 8 | 7 7 7\n",
        "    10 -> 9 9 | 8 7 7\n",
        "    9 -> '1' '2' | '1' '1' '3'\n",
        "    8 -> '2' '2' | '1' '1'\n",
        "    7 -> '2' '3' '1' | '3' '1' '2'\n",
        "\"\"\")\n",
        "\n",
        "cfg3h = CFG.fromstring(\"\"\"\n",
        "    22 -> 19 21 | 20 20 21\n",
        "    21 -> 17 18 17 | 17 17 18\n",
        "    20 -> 17 16 | 18 16\n",
        "    19 -> 18 17 | 16 17\n",
        "    18 -> 14 15 15 | 15 14 14 | 15 13 13\n",
        "    17 -> 15 13 15 | 13 14\n",
        "    16 -> 15 13 | 14 13\n",
        "    15 -> 11 11 10 | 10 12\n",
        "    14 -> 12 12 10 | 10 10 | 10 12 12\n",
        "    13 -> 11 10 | 12 11\n",
        "    12 -> 9 8 | 8 7 | 7 9\n",
        "    11 -> 7 9 9 | 7 7 | 8 7 7\n",
        "    10 -> 8 8 | 9 7 | 8 7 9\n",
        "    9 -> '1' '3' '3' | '2' '1' '3'\n",
        "    8 -> '1' '3' | '3' '3' '1' | '1' '2'\n",
        "    7 -> '1' '3' '1' | '1' '2' '3' | '2' '3' '2'\n",
        "\"\"\")\n",
        "\n",
        "cfg3g = CFG.fromstring(\"\"\"\n",
        "    22 -> 20 19 21 | 20 20 19 | 19 20\n",
        "    21 -> 18 16 | 16 16 18 | 16 16\n",
        "    20 -> 16 17 17 | 18 18 | 16 17\n",
        "    19 -> 18 16 17 | 18 17 16 | 17 17 16\n",
        "    18 -> 14 13 15 | 15 15 | 15 13\n",
        "    17 -> 15 14 | 14 15 13 | 14 13 14\n",
        "    16 -> 13 13 | 13 14 | 14 13 13\n",
        "    15 -> 12 11 | 12 10 10 | 10 11\n",
        "    14 -> 10 10 | 10 11 10 | 11 12\n",
        "    13 -> 11 11 | 11 11 11 | 10 12\n",
        "    12 -> 9 9 9 | 7 8 | 7 9\n",
        "    11 -> 8 9 7 | 9 7 | 8 8 9\n",
        "    10 -> 7 7 | 7 7 7 | 8 8 8\n",
        "    9 -> '2' '1' | '2' '3' | '2' '3' '3'\n",
        "    8 -> '3' '3' '1' | '1' '3' | '1' '3' '2'\n",
        "    7 -> '2' '2' | '1' '1' | '2' '3' '1'\n",
        "\"\"\")\n",
        "\n",
        "cfg3f = CFG.fromstring(\"\"\"\n",
        "    22 -> 20 20 | 21 19 19 | 20 19 21 | 20 21\n",
        "    21 -> 16 18 | 16 17 18 | 17 16 | 18 17\n",
        "    20 -> 17 16 18 | 16 17 | 16 16\n",
        "    19 -> 18 18 | 17 18 | 18 16 18\n",
        "    18 -> 13 15 | 15 13 13 | 14 15 13\n",
        "    17 -> 15 14 | 14 15 | 15 14 13\n",
        "    16 -> 14 14 | 14 13 | 13 15 13 | 15 15\n",
        "    15 -> 12 12 11 | 10 10 | 11 11 10 | 10 11 11\n",
        "    14 -> 10 12 12 | 12 11 | 12 10 12 | 10 12\n",
        "    13 -> 10 12 11 | 12 11 12 | 11 12\n",
        "    12 -> 8 8 9 | 9 8 | 7 9 7\n",
        "    11 -> 9 7 7 | 9 7 | 8 8\n",
        "    10 -> 7 9 9 | 9 7 9 | 8 9 9\n",
        "    9 -> '1' '1' | '3' '3' | '1' '2' '1'\n",
        "    8 -> '3' '3' '1' | '1' '2' | '3' '1' '1'\n",
        "    7 -> '3' '2' | '3' '1' '2' | '3' '2' '2' | '2' '2' '1'\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "ONLtZMWkF-IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#si el modelo tiene un block_size de tamaño x, n=x+1\n",
        "#determina el porcentaje de n-1-gramas diferentes en cada palabra del texto\n",
        "def diversity(text, n=17):\n",
        "    tokens = text.split()  # Assuming 'text' is a space-separated string\n",
        "    avg = 0\n",
        "    for tok in tokens:\n",
        "      if len(tok) > n:\n",
        "        n_grams = list(ngrams(tok, n))\n",
        "        unique_n_grams = set(n_grams)\n",
        "        avg_tok = len(unique_n_grams) / len(list(n_grams))\n",
        "        avg += avg_tok\n",
        "    return avg/len(tokens)\n",
        "\n",
        "def diversityNotCFG(text, n=17):\n",
        "  #num_trozos = (len(text) + n - 1) // n\n",
        "  # Dividir el texto en trozos\n",
        "  #trozos = [text[i * n:(i + 1) * n] for i in range(num_trozos)]\n",
        "  text_chunks = [text[i:i+n] for i in range(len(text)-n+1)]\n",
        "  unique_n_grams = set(text_chunks)\n",
        "  return len(unique_n_grams)/len(text_chunks)\n",
        "\n",
        "#vamos comprobando para cada frase de un texto si cumple las reglas de la cfg\n",
        "#devuelve el porcentaje de palabras del texto que las cumplen\n",
        "#evalúa la calidad de las predicciones del modelo\n",
        "def perplexity(text, grammar):\n",
        "    parser = RecursiveDescentParser(grammar)\n",
        "    frases = text.split()\n",
        "    valid = 0\n",
        "    for frase in frases:\n",
        "    #frases_posibles = []\n",
        "    #for frase in frases:\n",
        "    #    if len(frase) >= 138: #tamaño mínimo frase\n",
        "    #        frases_posibles.append(frase)\n",
        "    #for i, frasep in enumerate(frases_posibles):\n",
        "    #  if i < len(frases_posibles):\n",
        "      for tree in parser.parse(frase):\n",
        "    #print(frasep)\n",
        "        valid += 1\n",
        "            #print(tree)\n",
        "        break\n",
        "    print(f'Número de frases: {len(frases)}')\n",
        "    #print(f'Número de frases posibles: {len(frases_posibles)}')\n",
        "    print(f'Número de frases que cumplen las reglas: {valid}')\n",
        "    print(f'Perplejidad: {valid/len(frases)}')\n",
        "    #return valid/len(frases)\n",
        "\n",
        "print(len(gen_text))\n",
        "print(f'Diversity: {diversity(gen_text)}')\n",
        "#print(f'DiversityNotCFG: {diversityNotCFG(gen_text)}')\n",
        "#print(gen_text)\n",
        "perplexity(gen_text, cfg3b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdvlp2Z1NMmM",
        "outputId": "b1d7b109-bb1e-4b2c-8a0f-9c1deb58f5d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256500\n",
            "Diversity: 0.09829546204267123\n",
            "Número de frases: 712\n",
            "Número de frases que cumplen las reglas: 0\n",
            "Perplejidad: 0.0\n"
          ]
        }
      ]
    }
  ]
}